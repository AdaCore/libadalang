from copy import copy
import inspect
from itertools import count, chain
from common import gen_name, gen_names, get_type, null_constant, TOKEN_PREFIX

from c_api import CAPIType
from template_utils import TemplateEnvironment, common_renderer
from utils import (common_ancestor, memoized, copy_with, Colors,
                   GeneratedFunction)
import quex_tokens


def is_row(parser):
    return isinstance(parser, Row)


def is_enum(compiled_type):
    return issubclass(compiled_type, EnumType)


def is_ast_node(compiled_type):
    """Return whether `compiled_type` is an ASTNode in the generated code."""
    return issubclass(compiled_type, ASTNode)


###############
# AST HELPERS #
###############


def decl_type(ada_type):
    res = ada_type.name()
    return res.strip() + ("*" if ada_type.is_ptr else "")


render_template = common_renderer.update({
    'is_row':           is_row,
    'is_enum':          is_enum,
    'is_class':         inspect.isclass,
    'is_ast_node':      is_ast_node,
    'decl_type':        decl_type,
}).render


class ParserCodeContext(object):
    """
    ParserCodeContext encapsulates the return value of the
    Parser.generate_code primitive. A parser's code generation will return:

        pos_var_name: the name of the variable that points to the new
        position of the parser

        res_var_name: the name of the variable that points to the result of
        the parser

        code: The code generated by the parser, that will be encapsulated
        by the parent parser

        var_defs: A list of tuples of type (string, CompiledType). Each
        tuple represents a variable declaration that must be inserted at
        the top level (usually the function that will encapsulate this
        parser)
    """

    def __init__(self, pos_var_name, res_var_name, code, var_defs):
        self.pos_var_name = pos_var_name
        self.res_var_name = res_var_name
        self.code = code
        self.var_defs = var_defs


class CompiledType(object):
    """
    Base class used to describe types in the generated code.

    It is intended to be subclassed in order to create now compiled types.
    However, subclasses are not intended to be instantiated.
    """

    # Whether this type is handled through pointers only in the generated code.
    is_ptr = True

    def __init__(self):
        assert False, (
            'CompiledType subclasses are not meant to be instantiated'
        )

    @classmethod
    def add_to_context(cls, compile_ctx):
        """
        If needed, put bits into `compile_ctx` to implement this compiled type.

        Must be overriden in subclasses if needed.
        """
        pass

    @classmethod
    def needs_refcount(cls):
        raise NotImplementedError()

    @classmethod
    def name(cls):
        """
        Return a string to be used in code generation to reference this type.

        Must be overriden in subclasses.
        """
        raise NotImplementedError()

    @classmethod
    def nullexpr(cls):
        """
        Return a string to be used in code generation for "null" expressions.

        Must be overriden in subclasses.
        """
        raise NotImplementedError()

    @classmethod
    def c_type(cls, c_api_settings):
        """Return a CAPIType instance for this type

        Must be overriden in subclasses.
        """
        raise NotImplementedError()


class BasicType(CompiledType):
    """
    Base class used to describe simple types that do not need declaration code
    generation.
    """
    _name = None
    _nullexpr = None
    _external = False

    @classmethod
    def needs_refcount(cls):
        return False

    @classmethod
    def name(cls):
        return cls._name

    @classmethod
    def nullexpr(cls):
        return cls._nullexpr

    @classmethod
    def c_type(cls, c_api_settings):
        # Default implementation: tagged types should override this
        return CAPIType(c_api_settings, cls.name(), external=cls._external)


class BoolType(BasicType):
    is_ptr = False
    _name = get_type(bool)
    _nullexpr = "false"

    @classmethod
    def c_type(cls, c_api_settings):
        # "bool" is not a built-in in C: do not force users to pull
        # stdbool.h...
        return CAPIType(c_api_settings, 'int', external=True)


class LongType(BasicType):
    is_ptr = False
    _name = get_type(long)
    _nullexpr = "0"
    _external = True


class SourceLocationRangeType(BasicType):
    is_ptr = False
    _name = "SourceLocationRange"
    _nullexpr = "SourceLocationRange()"

    @classmethod
    def c_type(cls, c_api_settings):
        # "bool" is not a built-in in C: do not force users to pull
        # stdbool.h...
        return CAPIType(c_api_settings, 'source_location_range', 'struct')


class Token(BasicType):
    is_ptr = False
    _name = "Token"
    _nullexpr = "no_token"


class NoToken(Token):
    quex_token_name = "TERMINATION"


class Indent(object):
    KIND_REL_POS = 1
    KIND_TOKEN_POS = 2

    def __init__(self, kind, rel_pos=0, token_field_name=""):
        self.kind = kind
        self.rel_pos = rel_pos
        self.token_field_name = token_field_name


def indent_rel(pos=0):
    return Indent(Indent.KIND_REL_POS, rel_pos=pos)


def indent_token(field_name=""):
    return Indent(Indent.KIND_TOKEN_POS, token_field_name=field_name)


class Field(object):
    """
    Placeholder descriptors used to associate data to AST nodes (see below).
    """

    # Hack: the field declarations order in AST nodes matters.  The simple and
    # very handy syntax we use here for such declarations doesn't preserve this
    # order in Python2, however.  Waiting for the move to Python3, we use a
    # hack here: the following counter will help us to recover the declaration
    # order (assuming it is the same as the Field instantiation order).
    _counter = iter(count(0))

    def __init__(self, repr=True, indent=indent_rel()):
        """Create an AST node field.

        If `repr`, the field will be displayed when pretty-printing the
        embedding AST node.
        """
        self.repr = repr
        self._name = None

        if type(indent) == int:
            indent = indent_rel(indent)

        self.indent = indent

        self._index = next(self._counter)

    def _get_name(self):
        assert self._name
        return self._name

    def _set_name(self, name):
        assert isinstance(name, basestring)
        self._name = name

    name = property(_get_name, _set_name)

    def __repr__(self):
        return '<ASTNode {} Field({})>'.format(self._index, self._name)


class AstNodeMetaclass(type):
    """
    Internal metaclass for AST nodes, used to ease fields handling during code
    generation.
    """
    def __new__(mcs, name, bases, dct):
        assert len(bases) == 1, (
            "Multiple inheritance for AST nodes is not supported")

        fields = []

        # Associate a name to all fields and collect them into `field`...
        for fld_name, fld_value in dct.items():
            if isinstance(fld_value, Field):
                fld_value.name = fld_name
                fields.append(fld_value)

        # ... and then remove them as class members: we want them to be
        # stored in a single class member: the "field" one, being a list.
        for field in fields:
            dct.pop(field.name)

        # Hack to recover the order of field declarations.  See the Field class
        # definition for more details.
        dct['fields'] = sorted(fields, key=lambda f: f._index)

        # By default, ASTNode subtypes aren't abstract.
        dct['abstract'] = False

        return type.__new__(mcs, name, bases, dct)


def abstract(cls):
    """Decorator to tag an ASTNode subclass as abstract."""
    assert issubclass(cls, ASTNode)
    cls.abstract = True
    return cls


class ASTNode(CompiledType):
    """
    Base class for all user AST nodes.

    Subclasses can define new AST node types, but they also can be abstract
    themselves (to form a true tree of AST node types).  Each subclass can
    define a list of fields (see the above Field class), so that each concrete
    class' fields are the sum of all its subclass' fields plus its own.

    This base class defines utilities to emit native code for the AST node
    types: type declaration, type definition and type usage (to declare
    AST node variables).
    """

    abstract = False
    fields = []
    __metaclass__ = AstNodeMetaclass

    @classmethod
    def create_type_declaration(cls):
        """Return a forward type declaration for this AST node type."""
        return render_template('astnode_type_decl', cls=cls)

    @classmethod
    def needs_refcount(cls):
        return True

    @classmethod
    def create_type_definition(cls, compile_ctx):
        """
        Emit a type definition for this AST node type in
        `compile_ctx.types_definitions`, emit:
          - a class with all its fields and its methods;
          - a forward declaration for this AST node type's "nil" singleton;

        Also emit the implementation for the corresponding methods/singletons
        in `compile_ctx.body`.
        """
        base_class = cls.__bases__[0]

        # Some templates need all fields (inherited and not inherited) and some
        # need only not inherited ones.
        assert len(cls.get_types(compile_ctx)) == len(cls.get_fields()), (
            "{}: {} <-> {}".format(
                cls,
                cls.get_types(compile_ctx), cls.get_fields()
            )
        )
        all_field_decls = zip(cls.get_types(compile_ctx), cls.get_fields())
        cls_field_decls = zip(compile_ctx.ast_fields_types[cls], cls.fields)

        t_env = TemplateEnvironment(
            cls=cls,
            capi=compile_ctx.c_api_settings,
            all_field_decls=all_field_decls,
            cls_field_decls=cls_field_decls,
            types=compile_ctx.ast_fields_types[cls],
            base_name=base_class.name()
        )
        tdef = render_template('astnode_type_def', t_env)
        if cls.is_ptr:
            compile_ctx.types_definitions.append(tdef)
        else:
            compile_ctx.val_types_definitions.append(tdef)

        compile_ctx.body.append(render_template('astnode_type_impl', t_env))

    @classmethod
    def get_inheritance_chain(cls):
        """
        Return a list for all classes from ASTNode to `cls` in the inheritance
        chain.
        """
        return reversed([base_class for base_class in cls.mro()
                         if issubclass(base_class, ASTNode)])

    @classmethod
    def get_fields(cls):
        """
        Return the list of all the fields `cls` has, including its parents'.
        """
        fields = []
        for base_class in cls.get_inheritance_chain():
            fields.extend(base_class.fields)
        return fields

    @classmethod
    def get_types(cls, compile_ctx):
        """
        Return the list of types for all the fields `cls` has, inclusing its
        parents'.
        """
        types = []
        for base_class in cls.get_inheritance_chain():
            types.extend(compile_ctx.ast_fields_types[base_class])
        return types

    @classmethod
    def get_public_fields(cls, compile_ctx):
        """
        Return a (field, field type) list for all the fields that are
        readable through the public API for this node (excluding fields from
        parents).
        """
        # All fields are exported unless they hold a token.
        return [(field, field_type)
                for field, field_type in zip(
                    cls.get_fields(), cls.get_types(compile_ctx))
                if not issubclass(field_type, Token)]

    @classmethod
    def add_to_context(cls, compile_ctx):
        """
        Emit code to `compile_ctx` for this AST node type.  Do nothing if
        called more than once on a single class or if called on ASTNode itself.
        """
        if cls not in compile_ctx.types and cls != ASTNode:
            base_class = cls.__bases__[0]
            if issubclass(base_class, ASTNode) and base_class != ASTNode:
                base_class.add_to_context(compile_ctx)

            compile_ctx.types.add(cls)
            compile_ctx.types_declarations.append(
                cls.create_type_declaration())
            cls.create_type_definition(compile_ctx)

            primitives = []
            for field, field_type in cls.get_public_fields(compile_ctx):
                t_env = TemplateEnvironment(
                    capi=compile_ctx.c_api_settings,
                    astnode=cls,
                    field=field,
                    field_type=field_type,
                )
                primitives.append(GeneratedFunction(
                    render_template('c_astnode_field_access_decl', t_env),
                    render_template('c_astnode_field_access_impl', t_env),
                ))
            compile_ctx.c_astnode_primitives[cls] = primitives

    @classmethod
    def name(cls):
        """
        Return the name that will be used in code generation for this AST node
        type.
        """
        return cls.__name__

    @classmethod
    def repr_name(cls):
        """Return a name that will be used when serializing this AST node."""
        return getattr(cls, "_repr_name", cls.name())

    @classmethod
    def nullexpr(cls):
        """
        Return a value that can be considered as "null" for this AST node type.
        It indicates the absence of AST node.
        """
        if cls.is_ptr:
            return null_constant()
        else:
            return "nil_{0}".format(cls.name())

    @classmethod
    def c_type(cls, c_api_settings):
        return CAPIType(c_api_settings, 'node')


def resolve(parser):
    """
    :type parser: Parser|Token|ParserContainer
    :rtype: Parser
    """
    if isinstance(parser, Parser):
        return parser
    elif isinstance(parser, type) and issubclass(parser, Token):
        return TokClass(parser)
    elif isinstance(parser, Token):
        return Tok(parser)
    elif isinstance(parser, str):
        return Tok(parser)
    else:
        raise Exception("Cannot resolve parser {}".format(parser))


class Grammar(object):
    """
    Holder for parsing rules.

    Parsing rules can be added incrementally while referencing each other: this
    class will automatically resolve forward references when needed.
    """

    def __init__(self):
        self.rules = {}

    def add_rules(self, **kwargs):
        """
        Add rules to the grammar.  The keyword arguments will provide a name to
        rules.
        """
        for name, rule in kwargs.items():
            self.rules[name] = rule
            rule.set_name(name)
            rule.set_grammar(self)
            rule.is_root = True

    def __getattr__(self, rule_name):
        """Build and return a Defer parser that references the above rule."""
        return Defer(rule_name, lambda: self.rules[rule_name])


class Parser(object):
    """Base class for parsers building blocks."""

    # noinspection PyMissingConstructor
    def __init__(self):
        self._mod = None
        self.gen_fn_name = gen_name(self.__class__.__name__ + "_parse")
        self.grammar = None
        self.is_root = False
        self._name = ""

    @property
    def name(self):
        return self._name

    def discard(self):
        return False

    def needs_refcount(self):
        if self.get_type():
            return self.get_type().needs_refcount()
        return False

    def __or__(self, other):
        """Return a new parser that matches this one or `other`."""

        # Optimization: if we are building an `Or` parser out of other `Or`
        # parsers, flatten the result.

        # Here, we used to mutate existing parsers instead of cloning them.
        # This is bad since parsers can be shared, and user expect such
        # combinatory operations to create new parsers without affecting
        # existing ones.

        alternatives = []
        other_parser = resolve(other)

        if isinstance(self, Or):
            alternatives.extend(self.parsers)
        else:
            alternatives.append(self)

        if isinstance(other_parser, Or):
            alternatives.extend(other_parser.parsers)
        else:
            alternatives.append(other_parser)

        return Or(*alternatives)

    def __xor__(self, transform_fn):
        """
        :type transform_fn: (T) => U
        :rtype: Transform
        """
        return Transform(self, transform_fn)

    def set_grammar(self, grammar):
        """Associate `grammar` to this parser and to all its children."""
        for c in self.children():
            c.set_grammar(grammar)
        self.grammar = grammar

    def set_name(self, name):
        """
        Rename this parser and all its children so that `name` is part of the
        corresponding function in the generated code.
        """
        for c in self.children():
            if not c._name and not isinstance(c, Defer):
                c.set_name(name)

        self._name = name
        self.gen_fn_name = gen_name("{0}_{1}_parse".format(
            name, self.__class__.__name__.lower()))

    def is_left_recursive(self):
        """Return whether this parser is left-recursive."""
        return self._is_left_recursive(self.name)

    def _is_left_recursive(self, rule_name):
        """
        Private function used only by is_left_recursive, will explore the
        parser tree to verify whether the named parser with name rule_name is
        left recursive or not.
        """
        raise NotImplementedError()

    # noinspection PyMethodMayBeStatic
    def children(self):
        """
        Parsers are combined to create new and more complex parsers.  They make
        up a parser tree.  Return a list of children for this parser.

        Subclasses should override this method if they have children.
        """
        return []

    def compute_fields_types(self, compile_ctx):
        """
        Infer ASTNode's fields from this parsers tree.

        This method recurses over child parsers.  Parser subclasses must
        override this method if they contribute to fields typing.
        """
        for child in self.children():
            child.compute_fields_types(compile_ctx)

    def compile(self, compile_ctx):
        """
        Emit code for this parser as a function into the `compile_ctx` parser.

        :type compile_ctx: compile_context.CompileCtx
        """
        t_env = TemplateEnvironment()
        t_env._self = self

        # Don't emit code twice for the same parser.
        if self.gen_fn_name in compile_ctx.fns:
            return
        compile_ctx.fns.add(self.gen_fn_name)

        t_env.parser_context = (
            self.generate_code(compile_ctx=compile_ctx)
        )

        t_env.fn_profile = render_template('parser_fn_profile', t_env)
        t_env.fn_code = render_template('parser_fn_code', t_env)

        compile_ctx.body.append(t_env.fn_code)
        compile_ctx.fns_decls.append(t_env.fn_profile)

    def get_type(self):
        """
        Return a descriptor for the type this parser returns in the generated
        code.  It can be either the Token class or a CompiledType subtype.

        Subclasses must override this method.
        """
        raise NotImplementedError()

    def gen_code_or_fncall(self, compile_ctx, pos_name="pos"):
        """
        Return generated code for this parser into `compile_ctx`.

        `pos_name` is the name of a variable that contains the position of the
        next token in the lexer.

        Either the "parsing code" is returned, either it is emitted in a
        dedicated function and a call to it is returned instead.  This method
        relies on the subclasses-defined `generated_code` for "parsing code"
        generation.

        :rtype: ParserCodeContext
        """

        if self.name and compile_ctx.verbose:
            print "Compiling rule : {0}".format(
                Colors.HEADER + self.gen_fn_name + Colors.ENDC
            )

        # Users must be able to run parsers that implement a named rule, so
        # generate dedicated functions for them.
        if self.is_root:

            # The call to compile will add the declaration and the definition
            # (body) of the function to the compile context
            self.compile(compile_ctx)

            # Generate a call to the previously compiled function, and return
            # the context corresponding to this call
            pos, res = gen_names("fncall_pos", "fncall_res")
            fncall_block = render_template(
                'parser_fncall',
                _self=self, pos_name=pos_name,
                pos=pos, res=res
            )

            return ParserCodeContext(
                pos_var_name=pos,
                res_var_name=res,
                code=fncall_block,
                var_defs=[
                    (pos, LongType),
                    (res, self.get_type())
                ]
            )

        else:
            return self.generate_code(compile_ctx, pos_name)

    def generate_code(self, compile_ctx, pos_name="pos"):
        """
        Return generated code for this parser into `compile_ctx`.

        Subclasses must override this method.  It is a low-level routine used
        by the `gen_code_or_fncall` method.  See above for arguments meaning.
        """
        raise NotImplementedError()


class Tok(Parser):
    """Parser that matches a specific token."""

    def __repr__(self):
        return "Tok({0})".format(repr(self.val))

    def discard(self):
        return not self.keep

    def _is_left_recursive(self, rule_name):
        return False

    def __init__(self, val, keep=False):
        """
        Create a parser that matches `tok`.
        """
        Parser.__init__(self)
        self.val = val
        self.keep = keep
        self.token_kind = (
            TOKEN_PREFIX + quex_tokens.token_map.str_to_names[val]
        )

    def get_type(self):
        return Token

    def generate_code(self, compile_ctx, pos_name="pos"):

        # Generate the code to match the token of kind 'token_kind', and return
        # the corresponding context
        pos, res = gen_names("tk_pos", "tk_res")
        code = render_template(
            'tok_code',
            _self=self, pos_name=pos_name,
            pos=pos, res=res, token_kind=self.token_kind
        )

        return ParserCodeContext(
            pos_var_name=pos,
            res_var_name=res,
            code=code,
            var_defs=[(pos, LongType), (res, Token)]
        )


class TokClass(Parser):
    """Parser that matches a class of tokens."""

    def _is_left_recursive(self, rule_name):
        return False

    def discard(self):
        return not self.keep

    def __repr__(self):
        return "TokClass({0})".format(self.tok_class.__name__)

    def __init__(self, tok_class, keep=False):
        """
        Create a parser that matches all tokens in `tok_class`.
        """
        Parser.__init__(self)
        self.keep = keep
        self.tok_class = tok_class

    def get_type(self):
        return Token

    def generate_code(self, compile_ctx, pos_name="pos"):

        # Generate the code to match the token of kind 'token_kind', and return
        # the corresponding context
        pos, res = gen_names("tk_class_pos", "tk_class_res")
        token_kind = TOKEN_PREFIX + self.tok_class.quex_token_name
        code = render_template(
            'tok_code',
            _self=self, pos_name=pos_name,
            pos=pos, res=res, token_kind=token_kind,
        )

        return ParserCodeContext(
            pos_var_name=pos,
            res_var_name=res,
            code=code,
            var_defs=[(pos, LongType), (res, Token)]
        )


class Or(Parser):
    """Parser that matches what the first sub-parser accepts."""

    def _is_left_recursive(self, rule_name):
        return any(parser._is_left_recursive(rule_name)
                   for parser in self.parsers)

    def __repr__(self):
        return "Or({0})".format(", ".join(repr(m) for m in self.parsers))

    def __init__(self, *parsers):
        """
        Create a parser that matches any thing that the first parser in
        `parsers` accepts.

        :type parsers: list[Parser|Token|type]
        """
        Parser.__init__(self)
        self.parsers = [resolve(m) for m in parsers]

        # Typing resolution for this parser is a recursive process.  So first
        # we need to prevent infinite recursions (because of recursive
        # grammars)...
        self.is_processing_type = False

        # ... and we want to memoize the result.
        self.cached_type = None

    def children(self):
        return self.parsers

    def get_type(self):
        if self.cached_type:
            return self.cached_type

        # Callers are already visiting this node, so we cannot return its type
        # right now.  Return None so that it doesn't contribute to type
        # resolution.
        if self.is_processing_type:
            return None

        try:
            self.is_processing_type = True
            types = set()
            for m in self.parsers:
                t = m.get_type()
                if t:
                    types.add(t)

            # There are two possibilities:
            #  - if all alternatives return AST nodes: then this parser's
            #    return type is the common ancestor for all of these.
            #  - otherwise, make sure that all alternatives return exactly the
            #    same type.
            if all(issubclass(t, ASTNode) for t in types):
                res = common_ancestor(*types)
            else:
                typs = list(types)
                assert all(type(t) == type(typs[0]) for t in typs)
                res = typs[0]

            self.cached_type = res
            return res
        finally:
            self.is_processing_type = False

    def generate_code(self, compile_ctx, pos_name="pos"):
        pos, res = gen_names('or_pos', 'or_res')
        t_env = TemplateEnvironment(
            _self=self,

            # List of ParserCodeContext instances for the sub-parsers,
            # encapsulating their results
            results=[
                m.gen_code_or_fncall(compile_ctx, pos_name)
                for m in self.parsers
            ],

            # Generate a name for the exit label (when one of the sub-parsers
            # has matched)
            exit_label=gen_name("Exit_Or"),

            pos=pos,
            res=res,

            # Final type of the result of the Or parser
            typ=decl_type(self.get_type())
        )

        code = render_template('or_code', t_env)

        return ParserCodeContext(
            pos_var_name=t_env.pos,
            res_var_name=t_env.res,
            code=code,

            # For var defs, we create a new list that is the concatenation of
            # all the sub parsers variable definitions, adding the Or parser's
            # own pos and res variables
            var_defs=list(chain(
                [(pos, LongType), (res, self.get_type())],
                *[sr.var_defs for sr in t_env.results]
            ))
        )


def always_make_progress(parser):
    """Return whether `parser` cannot match an empty sequence of tokens."""
    if isinstance(parser, List):
        return not parser.empty_valid or always_make_progress(parser.parser)
    return not isinstance(parser, (Opt, Null))


class Row(Parser):
    """Parser that matches a what sub-parsers match in sequence."""

    def _is_left_recursive(self, rule_name):
        for parser in self.parsers:
            res = parser._is_left_recursive(rule_name)
            if res:
                return True
            if always_make_progress(parser):
                break
        return False

    def __repr__(self):
        return "Row({0})".format(", ".join(repr(m) for m in self.parsers))

    def __init__(self, *parsers):
        """
        Create a parser that matches the sequence of matches for all
        sub-parsers in `parsers`.

        :type parsers: list[Parser|Token|type]
        """
        Parser.__init__(self)
        self.parsers = [resolve(m) for m in parsers]

        # The type this row returns is initialized either when assigning a
        # wrapper parser or when trying to get the type (though the get_type
        # method) while no wrapper has been assigned.
        self.typ = None

        self.components_need_inc_ref = True
        self.args = []
        self.allargs = []

    def assign_wrapper(self, parser):
        """Associate `parser` as a wrapper for this Row.

        Note that a Row can have at most only one wrapper, so this does nothing
        if this Row is a root parser.
        """
        assert not self.is_root and not self.typ, (
            "Row parsers do not represent a concrete result. They must be used"
            " by a parent parser, such as Extract or Transform."
        )

        self.typ = parser.get_type()

    def children(self):
        return self.parsers

    def get_type(self):
        # A Row parser never yields a concrete result itself.
        return None

    def generate_code(self, compile_ctx, pos_name="pos"):
        """ :type compile_ctx: compile_context.CompileCtx """
        t_env = TemplateEnvironment(pos_name=pos_name)
        t_env._self = self

        t_env.pos, t_env.res, t_env.did_fail = gen_names(
            "row_pos", "row_res", "row_did_fail"
        )
        decls = [(t_env.pos, LongType), (t_env.did_fail, BoolType)]

        t_env.subresults = list(gen_names(*[
            "row_subres_{0}".format(i)
            for i in range(len(self.parsers))
        ]))
        t_env.exit_label = gen_name("row_exit_label")

        self.args = [r for r, m in zip(t_env.subresults, self.parsers)
                     if not m.discard()]
        self.allargs = [r for r, m in zip(t_env.subresults, self.parsers)]

        bodies = []
        for i, (parser, subresult) in enumerate(zip(self.parsers,
                                                    t_env.subresults)):
            t_subenv = TemplateEnvironment(
                t_env, parser=parser, subresult=subresult, i=i,
                parser_context=parser.gen_code_or_fncall(
                    compile_ctx, t_env.pos
                )
            )
            decls += t_subenv.parser_context.var_defs
            if not parser.discard():
                decls.append((subresult, parser.get_type()))

            bodies.append(render_template('row_submatch', t_subenv))

        code = render_template('row_code', t_env, body='\n'.join(bodies))

        return ParserCodeContext(
            pos_var_name=t_env.pos,
            res_var_name=t_env.res,
            code=code,
            var_defs=decls
        )

    def __rshift__(self, index):
        """
        Return a parser that matches `self` and that discards everything except
        the `index`th field in the row.
        """
        return Extract(self, index)


# We want structural equality on lists whose elements have the same types.
# Memoization is one way to make sure that, for each CompiledType subclass X:
#    list_type(X) == list_type(X)
@memoized
def list_type(element_type):
    """
    Return an ASTNode subclass that represent a list of `element_type`.
    """

    # List types do not need generated code for their declaration since they
    # are instantiations of a generic "ASTList" type, which inherits ASTNode
    # (so they can be considered as ASTNode).

    return type(
        '{}ListType'.format(element_type.name()),
        (ASTNode, ),
        {
            'is_ptr':   True,
            'name':     classmethod(
                lambda cls: render_template('list_type', el_type=element_type)
            ),
            'nullexpr': classmethod(lambda cls: null_constant()),
        }
    )


class List(Parser):
    """Parser that matches a list.  A sub-parser matches list items."""

    def _is_left_recursive(self, rule_name):
        res = self.parser._is_left_recursive(rule_name)
        assert not(
            res and (self.empty_valid
                     or not always_make_progress(self.parser))
        )
        return res

    def __repr__(self):
        return "List({0})".format(
            repr(self.parser) + (", sep={0}".format(self.sep)
                                 if self.sep else "")
        )

    def __init__(self, parser, sep=None, empty_valid=False, revtree=None):
        """
        Create a parser that matches a list of elements.

        Each element will be matched by `parser`.  If `sep` is provided, it is
        a parser that is used to match separators between elements.

        By default, this parser will not match empty sequences but it will if
        `empty_valid` is True.

        If `revtree` is provided, it must be an ASTNode subclass.  It is then
        used to fold the list into a binary tree.

        :type sep: Token|string
        :type empty_valid: bool
        """
        Parser.__init__(self)
        self.parser = resolve(parser)
        self.sep = resolve(sep) if sep else None
        self.empty_valid = empty_valid
        self.revtree_class = revtree

        if empty_valid:
            assert not self.revtree_class

    def children(self):
        return [self.parser]

    def get_type(self):
        if self.revtree_class:
            return common_ancestor(self.parser.get_type(), self.revtree_class)
        else:
            return list_type(self.parser.get_type())

    def compute_fields_types(self, compile_ctx):
        Parser.compute_fields_types(self, compile_ctx)

        # If this parser does no folding, it does not contribute itself to
        # fields typing, so we can stop here.
        if not self.revtree_class:
            return

        assert len(self.revtree_class.get_fields()) == 2, (
            "For folding, revtree classes must have two fields"
        )
        assert len(self.revtree_class.fields) == 2, (
            "Inheritance is not supported for revtree classes"
        )

        compile_ctx.set_ast_fields_types(
            self.revtree_class, [self.get_type()] * 2
        )

    def generate_code(self, compile_ctx, pos_name="pos"):
        """:type compile_ctx: compile_context.CompileCtx"""

        compile_ctx.generic_vectors.add(self.parser.get_type().name())
        cpos = gen_name("lst_cpos")
        parser_context = self.parser.gen_code_or_fncall(compile_ctx, cpos)
        sep_context = (
            self.sep.gen_code_or_fncall(compile_ctx, cpos)
            if self.sep else
            ParserCodeContext(None, None, None, [])
        )

        if self.revtree_class:
            self.revtree_class.add_to_context(compile_ctx)

        t_env = TemplateEnvironment(
            pos_name=pos_name,
            _self=self,
            pos=gen_name("lst_pos"),
            res=gen_name("lst_res"),
            cpos=cpos,
            parser_context=parser_context,
            sep_context=sep_context
        )

        decls = [
            (t_env.pos, LongType),
            (t_env.res, self.get_type()),
            (t_env.cpos, LongType),
        ] + parser_context.var_defs + sep_context.var_defs

        return ParserCodeContext(
            pos_var_name=t_env.pos,
            res_var_name=t_env.res,
            code=render_template('list_code', t_env),
            var_defs=decls
        )


class Opt(Parser):
    """
    Parser that matches something if possible or that matches an empty sequence
    otherwise.
    """

    def _is_left_recursive(self, rule_name):
        return self.parser._is_left_recursive(rule_name)

    def __repr__(self):
        return "Opt({0})".format(self.parser)

    def __init__(self, parser, *parsers):
        """
        Create a parser that matches `parser` and then `parsers` if possible or
        matches an empty sequence otherwise.  The result is equivalent to:

            Opt(Row(parser, *parsers)).
        """
        Parser.__init__(self)
        self._booleanize = False
        self.contains_anonymous_row = bool(parsers)
        self.parser = Row(parser, *parsers) if parsers else resolve(parser)

    def as_bool(self):
        self._booleanize = True
        if self.contains_anonymous_row:
            # What the sub-parser will match will not be returned, so there is
            # no need to generate an anonymous row type.  Tell so to the
            # Row sub-parser.
            assert isinstance(self.parser, Row)
            self.parser.assign_wrapper(self)
        return self

    def children(self):
        return [self.parser]

    def get_type(self):
        return BoolType if self._booleanize else self.parser.get_type()

    def generate_code(self, compile_ctx, pos_name="pos"):
        parser_context = copy(
            self.parser.gen_code_or_fncall(compile_ctx, pos_name)
        )

        t_env = TemplateEnvironment(
            pos_name=pos_name,
            _self=self,
            bool_res=gen_name("opt_bool_res"),
            parser_context=parser_context
        )

        return copy_with(
            parser_context,
            code=render_template('opt_code', t_env),
            res_var_name=(t_env.bool_res if self._booleanize
                          else parser_context.res_var_name),
            var_defs=parser_context.var_defs + ([(t_env.bool_res, BoolType)]
                                                if self._booleanize else [])
        )

    def __rshift__(self, index):
        """Same as Row.__rshift__."""
        m = self.parser
        assert isinstance(m, Row)
        return Opt(Extract(m, index))


class Extract(Parser):
    """
    Wrapper parser used to discard everything from a Row parser except a single
    field in it.
    """

    def _is_left_recursive(self, rule_name):
        return self.parser._is_left_recursive(rule_name)

    def __repr__(self):
        return "{0} >> {1}".format(self.parser, self.index)

    def __init__(self, parser, index):
        """
        :param Row parser: The parser that will serve as target for
        extract operation
        :param int index: The index you want to extract from the row
        """
        Parser.__init__(self)
        self.parser = parser
        self.index = index
        assert isinstance(self.parser, Row)
        self.parser.components_need_inc_ref = False

    def children(self):
        return [self.parser]

    def get_type(self):
        return self.parser.parsers[self.index].get_type()

    def generate_code(self, compile_ctx, pos_name="pos"):
        self.parser.assign_wrapper(self)

        return copy_with(
            self.parser.gen_code_or_fncall(compile_ctx, pos_name),
            res_var_name=self.parser.allargs[self.index]
        )


class Discard(Parser):
    """Wrapper parser used to discard the match."""

    def discard(self):
        return True

    def _is_left_recursive(self, rule_name):
        return self.parser._is_left_recursive(rule_name)

    def __repr__(self):
        return "Discard({0})".format(self.parser)

    def __init__(self, parser):
        Parser.__init__(self)

        parser = resolve(parser)
        if isinstance(parser, Row):
            parser.assign_wrapper(self)

        self.parser = parser

    def children(self):
        return [self.parser]

    def get_type(self):
        # Discard parsers return nothing!
        return None

    def generate_code(self, compile_ctx, pos_name="pos"):
        return self.parser.gen_code_or_fncall(compile_ctx, pos_name)


class Defer(Parser):
    """Stub parser used to implement forward references."""

    @property
    def parser(self):
        if not self._parser:
            self._parser = self.parser_fn()
        return self._parser

    @property
    def name(self):
        # Don't rely on `self.parser` since it may not be available right now
        # (that's why it is deferred in the first place).
        return self.rule_name

    def _is_left_recursive(self, rule_name):
        return self.name == rule_name

    def __repr__(self):
        return "Defer({0})".format(self.name)

    def __init__(self, rule_name, parser_fn):
        """
        Create a stub parser.

        `rule_name` must be the name of the deferred parser (used for
        pretty-printing).  `parser_fn` must be a callable that returns the
        referenced parser.
        """
        Parser.__init__(self)
        self.rule_name = rule_name
        self.parser_fn = parser_fn
        self._parser = None
        ":type: Parser"

    def get_type(self):
        return self.parser.get_type()

    def generate_code(self, compile_ctx, pos_name="pos"):
        return self.parser.gen_code_or_fncall(compile_ctx, pos_name=pos_name)


class Transform(Parser):
    """Wrapper parser for a Row parser used to instantiate an AST node."""

    def _is_left_recursive(self, rule_name):
        return self.parser._is_left_recursive(rule_name)

    def __repr__(self):
        return "{0} ^ {1}".format(self.parser, self.typ.name())

    def __init__(self, parser, typ):
        """
        Create a Transform parser wrapping `parser` and that instantiates AST
        nodes whose type is `typ`.
        """
        Parser.__init__(self)
        assert isinstance(typ, ASTNode) or issubclass(typ, ASTNode)

        self.parser = parser
        self.typ = typ
        self._is_ptr = typ.is_ptr

    def children(self):
        return [self.parser]

    def get_type(self):
        return self.typ

    def compute_fields_types(self, compile_ctx):
        # Gather field types that come from all child parsers.
        fields_types = (
            # There are multiple fields for Row parsers.
            [
                parser.get_type()
                for parser in self.parser.parsers
                if not parser.discard()
            ]
            if isinstance(self.parser, Row) else
            [self.parser.get_type()]
        )
        assert all(t for t in fields_types), (
            "Internal error when computing field types for {}:"
            " some are None: {}".format(self.typ, fields_types)
        )

        # Then dispatch these types to all the fields distributed amongst the
        # ASTNode hierarchy.
        for cls in self.typ.get_inheritance_chain():
            compile_ctx.set_ast_fields_types(
                cls, fields_types[:len(cls.fields)]
            )
            fields_types = fields_types[len(cls.fields):]

        Parser.compute_fields_types(self, compile_ctx)

    def generate_code(self, compile_ctx, pos_name="pos"):
        """
        :type compile_ctx: compile_context.CompileCtx
        """

        if isinstance(self.parser, Row):
            self.parser.assign_wrapper(self)

        self.typ.add_to_context(compile_ctx)
        compile_ctx.diag_types.append(self.typ)

        parser_context = self.parser.gen_code_or_fncall(compile_ctx, pos_name)
        ":type: ParserCodeContext"

        t_env = TemplateEnvironment(
            _self=self,
            # The template needs the compiler context to retrieve the types of
            # the tree fields (required by get_types())
            _compile_ctx=compile_ctx,
            parser_context=parser_context,
            args=(
                self.parser.args
                if isinstance(self.parser, Row) else
                [parser_context.res_var_name]
            ),
            res=gen_name("transform_res"),
        )

        return copy_with(
            parser_context,
            res_var_name=t_env.res,
            var_defs=parser_context.var_defs + [
                (t_env.res, self.get_type()),
            ],
            code=render_template(
                'transform_code', t_env, pos_name=pos_name
            )
        )


class Null(Parser):
    """Parser that matches the empty sequence and that yields no AST node."""

    def __init__(self, result_type):
        """
        Create a new Null parser.  `result_type` is either a CompiledType
        subclass that defines what nullexpr this parser returns, either a
        Parser subclass' instance.  In the latter case, this parser will return
        the same type as the other parser.
        """
        Parser.__init__(self)
        if isinstance(result_type, (CompiledType, Parser)):
            self.typ = result_type
        elif issubclass(result_type, CompiledType):
            self.typ = result_type
        else:
            raise TypeError(
                'Invalid result type for Null parser: {}'.format(result_type))

    def children(self):
        return []

    def _is_left_recursive(self, rule_name):
        return False

    def __repr__(self):
        return "Null"

    def generate_code(self, compile_ctx, pos_name="pos"):
        typ = self.get_type()
        if isinstance(typ, ASTNode):
            self.get_type().add_to_context(compile_ctx)
        res = gen_name("null_res")
        code = render_template('null_code', _self=self, res=res)

        return ParserCodeContext(
            pos_name,
            res,
            code,
            [(res, self.get_type())]
        )

    def get_type(self):
        return (
            self.typ.get_type()
            if isinstance(self.typ, Parser) else
            self.typ
        )


class EnumType(CompiledType):
    """
    Base class for compiled types that hold a single value in a set of possible
    ones.

    Subclasses must override the `alternatives` member to hold a list of
    distinct strings that represent the set of possibilities.  They represent
    the compiled type.

    Instances represent either the enum type itself in the generated code or a
    particular enum value.
    """

    is_ptr = False
    alternatives = []

    @classmethod
    def needs_refcount(cls):
        return False

    # noinspection PyMissingConstructor
    def __init__(self, alt):
        """Create a value that represent one of the enum alternatives."""
        # CompiledType are not usually supposed to be instantiated.  EnumType
        # is an exception to this rule, so do not call CompiledType.__init__.
        assert alt in self.alternatives
        self.alt = alt

    @classmethod
    def name(cls):
        return cls.__name__

    @classmethod
    def add_to_context(cls, compile_ctx):
        if cls not in compile_ctx.types:
            compile_ctx.types.add(cls)
            compile_ctx.types_declarations.append(
                render_template('enum_type_decl', cls=cls)
            )
            compile_ctx.body.append(
                render_template('enum_type_impl', cls=cls)
            )
            compile_ctx.c_astnode_field_types[cls] = render_template(
                'enum_c_type_decl',
                cls=cls,
                capi=compile_ctx.c_api_settings
            )

    @classmethod
    def nullexpr(cls):
        return cls.name() + "::uninitialized"

    @classmethod
    def c_type(cls, c_api_settings):
        return CAPIType(c_api_settings, cls.name(), 'enum')

    @classmethod
    def c_alternatives(cls, c_api_settings):
        """
        Return the sequence of names to use for alternatives in the C API
        """
        # Before wrapping, names can have "_" suffixes or prefixes in order to
        # avoid clashes with keywords. This is not needed anymore after
        # wrapping so remove it to have pleasant names.
        return [
            c_api_settings.get_name(
                "{}_{}".format(cls.name(), alt.strip('_'))
            )
            for alt in cls.alternatives
        ]


class Enum(Parser):
    """Wrapper parser used to returns an enumeration value for an match."""

    def _is_left_recursive(self, rule_name):
        if self.parser:
            return self.parser._is_left_recursive(rule_name)
        return False

    def __repr__(self):
        return "Enum({0}, {1})".format(self.parser, self.enum_type_inst)

    def __init__(self, parser, enum_type_inst):
        """
        Create a wrapper parser around `parser` that returns `enum_type_inst`
        (an EnumType subclass instance) when matching.
        """
        Parser.__init__(self)
        self.parser = resolve(parser) if parser else None
        ":type: Parser|Row"

        self.enum_type_inst = enum_type_inst

    def children(self):
        return []

    def get_type(self):
        return type(self.enum_type_inst)

    def generate_code(self, compile_ctx, pos_name="pos"):

        # The sub-parser result will not be used.  We have to notify it if it's
        # a Row so it does not try to generate an anonymous row type.
        if isinstance(self.parser, Row):
            self.parser.assign_wrapper(self)

        self.enum_type_inst.add_to_context(compile_ctx)

        parser_context = (
            copy(self.parser.gen_code_or_fncall(compile_ctx, pos_name))
            if self.parser
            else ParserCodeContext(
                pos_var_name=pos_name,
                res_var_name="",
                code="",
                var_defs=[]
            )
        )

        env = TemplateEnvironment(
            _self=self,
            res=gen_name("enum_res"),
            parser_context=parser_context

        )

        return copy_with(
            parser_context,
            res_var_name=env.res,
            code=render_template('enum_code', env),
            var_defs=parser_context.var_defs + [(env.res, self.get_type())]
        )


_ = Discard
